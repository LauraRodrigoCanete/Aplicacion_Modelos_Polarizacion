{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20dd9426-3a7a-4d15-a995-93efafa83f4f",
   "metadata": {},
   "source": [
    "## Aplicación Práctica de los Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d77b3f",
   "metadata": {},
   "source": [
    "###### Créditos: para la implementación de este código se ha hecho uso en parte de la inteligencia artificial ChatGPT 4.o y Gemini 2.5 Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43359a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer y preparar los datos, leer dfs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tweets16_dem = pd.read_csv('tweets16_dem.csv')\n",
    "tweets16_rep = pd.read_csv('tweets16_rep.csv')\n",
    "\n",
    "tweets20_dem = pd.read_csv('tweets20_dem.csv')\n",
    "tweets20_rep = pd.read_csv('tweets20_rep.csv')\n",
    "\n",
    "tweets24_dem = pd.read_csv('tweets24_dem.csv')\n",
    "tweets24_rep = pd.read_csv('tweets24_rep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d293cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobar tamaños y usuarios en común\n",
    "print('tweets16_dem:', tweets16_dem.shape)\n",
    "print('tweets16_rep:', tweets16_rep.shape)\n",
    "print('tweets20_dem:', tweets20_dem.shape)\n",
    "print('tweets20_rep:', tweets20_rep.shape)\n",
    "print('tweets24_dem:', tweets24_dem.shape)\n",
    "print('tweets24_rep:', tweets24_rep.shape)\n",
    "\n",
    "tweets16_dem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobar que no haya ningún nan ni none en todo el df\n",
    "print(tweets16_dem.isnull().sum())\n",
    "print(tweets16_rep.isnull().sum())\n",
    "print(tweets20_dem.isnull().sum())\n",
    "print(tweets20_rep.isnull().sum())\n",
    "print(tweets24_dem.isnull().sum())\n",
    "print(tweets24_rep.isnull().sum())\n",
    "\n",
    "print(tweets16_dem.isna().sum())\n",
    "print(tweets16_rep.isna().sum())\n",
    "print(tweets20_dem.isna().sum())\n",
    "print(tweets20_rep.isna().sum())\n",
    "print(tweets24_dem.isna().sum())\n",
    "print(tweets24_rep.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61397b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobar usuarios en común de la columna \"User\"\n",
    "print('tweets16_dem:', len(tweets16_dem['User'].unique()))\n",
    "print('tweets16_rep:', len(tweets16_rep['User'].unique()))\n",
    "print('tweets20_dem:', len(tweets20_dem['User'].unique()))\n",
    "print('tweets20_rep:', len(tweets20_rep['User'].unique()))\n",
    "print('tweets24_dem:', len(tweets24_dem['User'].unique()))\n",
    "print('tweets24_rep:', len(tweets24_rep['User'].unique()))\n",
    "\n",
    "# Hacemos las intersecciones de los usuarios\n",
    "inter_16 = set(tweets16_dem['User'].unique()).intersection(set(tweets16_rep['User'].unique()))\n",
    "inter_20 = set(tweets20_dem['User'].unique()).intersection(set(tweets20_rep['User'].unique()))\n",
    "inter_24 = set(tweets24_dem['User'].unique()).intersection(set(tweets24_rep['User'].unique()))\n",
    "print('inter_16:', len(inter_16))\n",
    "print('inter_20:', len(inter_20))\n",
    "print('inter_24:', len(inter_24))\n",
    "\n",
    "# Comprobar si hay usuarios en común entre los años haciendo las intersecciones entre inter_16, inter_20 e inter_24\n",
    "inter_16_20 = inter_16.intersection(inter_20)\n",
    "inter_16_24 = inter_16.intersection(inter_24)\n",
    "inter_20_24 = inter_20.intersection(inter_24)\n",
    "print('inter_16_20:', len(inter_16_20))\n",
    "print('inter_16_24:', len(inter_16_24))\n",
    "print('inter_20_24:', len(inter_20_24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dce710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sacar número de usuarios totales por año\n",
    "\n",
    "print('users 16: ' + str(len(tweets16_dem['User'].unique()) + len(tweets16_rep['User'].unique()) - len(inter_16)))\n",
    "print('users 20: ' + str(len(tweets20_dem['User'].unique()) + len(tweets20_rep['User'].unique()) - len(inter_20)))\n",
    "print('users 24: ' + str(len(tweets24_dem['User'].unique()) + len(tweets24_rep['User'].unique()) - len(inter_24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vote_intention(tweets_rep, tweets_dem):\n",
    "    # Valoración y número de tweets para los republicanos en 2016\n",
    "    rep_summary = tweets_rep.groupby('User').agg(\n",
    "        u_R=('Label', 'mean'),\n",
    "        m_R=('Label', 'count')\n",
    "    )\n",
    "\n",
    "    # Valoración y número de tweets para los demócratas en 2016\n",
    "    dem_summary = tweets_dem.groupby('User').agg(\n",
    "        u_D=('Label', 'mean'),\n",
    "        m_D=('Label', 'count')\n",
    "    )\n",
    "\n",
    "    # Unimos ambos resúmenes teniendo en cuenta todos los usuarios (outer join)\n",
    "    vote_df = rep_summary.join(dem_summary, how='outer').fillna(0)\n",
    "\n",
    "    # Calculamos vote_intention usando la fórmula:\n",
    "    # vote_intention = (u_R*m_R - u_D*m_D) / (m_R + m_D)\n",
    "    # Para el caso en que (m_R + m_D)==0, definimos vote_intention = 0\n",
    "    vote_df['vote_intention'] = vote_df.apply(\n",
    "        lambda row: (row['u_R']*row['m_R'] - row['u_D']*row['m_D']) / (row['m_R'] + row['m_D'])\n",
    "                    if (row['m_R'] + row['m_D']) != 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return vote_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_df_16 = build_vote_intention(tweets16_rep, tweets16_dem)\n",
    "V_df_20 = build_vote_intention(tweets20_rep, tweets20_dem)\n",
    "V_df_24 = build_vote_intention(tweets24_rep, tweets24_dem)\n",
    "\n",
    "print('V_16:', V_df_16)\n",
    "print('V_20:', V_df_20)\n",
    "print('V_24:', V_df_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccea250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# el data_table: Una estructura de tabla con columnas 'nombre de usuario', 'tweet', 'etiqueta'. Las etiquetas son 'dem', 'neu', o 'rep'.\n",
    "# Cogemos las tablas originales y en la de rep si Label == 1 la cambiamos a 'rep' y si es -1 a 'dem' y si es 0 a 'neu' y en la tabla dem al revés\n",
    "\n",
    "def build_data_table(tweets_rep, tweets_dem):\n",
    "    # Cambiamos las etiquetas de los tweets republicanos\n",
    "    tweets_rep['Label'] = tweets_rep['Label'].replace({1: 'rep', -1: 'dem', 0: 'neu'})\n",
    "    # Cambiamos las etiquetas de los tweets demócratas\n",
    "    tweets_dem['Label'] = tweets_dem['Label'].replace({-1: 'rep', 1: 'dem', 0: 'neu'})\n",
    "\n",
    "    # Concatenamos ambos dataframes\n",
    "    data_table = pd.concat([tweets_rep, tweets_dem], ignore_index=True)\n",
    "\n",
    "    return data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table_16 = build_data_table(tweets16_rep, tweets16_dem)\n",
    "data_table_20 = build_data_table(tweets20_rep, tweets20_dem)\n",
    "data_table_24 = build_data_table(tweets24_rep, tweets24_dem)\n",
    "print('data_table_16:', data_table_16)\n",
    "print('data_table_20:', data_table_20)\n",
    "print('data_table_24:', data_table_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparamos los parámetros de los índices\n",
    "\n",
    "# vote_intention\n",
    "V_16 = V_df_16['vote_intention'].values\n",
    "V_20 = V_df_20['vote_intention'].values\n",
    "V_24 = V_df_24['vote_intention'].values\n",
    "\n",
    "# vote_intention_[0,1] (la traslación de (vote_intention + 1)/2 )\n",
    "V_16_traslacion = (V_df_16['vote_intention'] + 1) / 2\n",
    "V_20_traslacion = (V_df_20['vote_intention'] + 1) / 2\n",
    "V_24_traslacion = (V_df_24['vote_intention'] + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos la distribución graficamente de vote_intention\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(V_16, bins=20, alpha=0.5, label='2016')\n",
    "plt.xlabel('Vote Intention')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Vote Intention Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c64f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(V_20, bins=20, alpha=0.5, label='2020')\n",
    "plt.xlabel('Vote Intention')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Vote Intention Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db66df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(V_24, bins=20, alpha=0.5, label='2024')\n",
    "plt.xlabel('Vote Intention')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Vote Intention Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2abed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print('data_table_16:')\n",
    "data_table_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf414297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# muestra de las data_table usuarios que tengan algunos tweets con etiquetas diferentes\n",
    "\n",
    "# Filtrar los usuarios que tienen tweets con al menos dos etiquetas diferentes\n",
    "users_with_both = (\n",
    "    data_table_16.groupby('User')['Label']\n",
    "    .apply(lambda etiquetas: len(set(etiquetas)) >= 2)\n",
    ")\n",
    "selected_users = users_with_both[users_with_both].index.tolist()\n",
    "subset_data_table = data_table_16[data_table_16['User'].isin(selected_users)]\n",
    "\n",
    "print(\"Usuarios con tweets de al menos dos etiquetas diferentes:\")\n",
    "print(len(subset_data_table['User'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_both = (\n",
    "    data_table_20.groupby('User')['Label']\n",
    "    .apply(lambda etiquetas: len(set(etiquetas)) >= 2)\n",
    ")\n",
    "selected_users = users_with_both[users_with_both].index.tolist()\n",
    "subset_data_table = data_table_20[data_table_20['User'].isin(selected_users)]\n",
    "\n",
    "print(\"Usuarios con tweets de al menos dos etiquetas diferentes:\")\n",
    "print(len(subset_data_table['User'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b85687",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_both = (\n",
    "    data_table_24.groupby('User')['Label']\n",
    "    .apply(lambda etiquetas: len(set(etiquetas)) >= 2)\n",
    ")\n",
    "selected_users = users_with_both[users_with_both].index.tolist()\n",
    "subset_data_table = data_table_24[data_table_24['User'].isin(selected_users)]\n",
    "\n",
    "print(\"Usuarios con tweets de al menos dos etiquetas diferentes:\")\n",
    "print(len(subset_data_table['User'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdced5ab",
   "metadata": {},
   "source": [
    "#### Índice de Polarización Gravitatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calcular_IPG(V):\n",
    "    \"\"\"\n",
    "    Calcula el Índice de Polarización Gravitatoria (IPG) para un vector\n",
    "    discreto de opiniones V.\n",
    "\n",
    "    Args:\n",
    "        V: Un vector (lista o array de numpy) de valores numéricos\n",
    "           discretos, representando opiniones en el rango [-1, 1].\n",
    "\n",
    "    Returns:\n",
    "        El valor del IPG calculado, o 0 si el vector de entrada está vacío.\n",
    "    \"\"\"\n",
    "    N = len(V)\n",
    "    if N == 0:\n",
    "        return 0\n",
    "\n",
    "    # --- Paso 1: Calcular probabilidades de masa ---\n",
    "    # Contar el número de usuarios con opiniones < 0, == 0, > 0\n",
    "    count_neg = sum(1 for v in V if v < 0)\n",
    "    count_zero = sum(1 for v in V if v == 0)\n",
    "    count_pos = sum(1 for v in V if v > 0)\n",
    "\n",
    "    # Calcular las probabilidades de masa P(X < 0), P(X = 0), P(X > 0)\n",
    "    # Las probabilidades son proporciones sobre el total de usuarios.\n",
    "    prob_neg = count_neg / N\n",
    "    prob_zero = count_zero / N\n",
    "    prob_pos = count_pos / N\n",
    "    prob_not_zero = prob_neg + prob_pos # O 1.0 - prob_zero\n",
    "\n",
    "    # --- Paso 2: Calcular A+ y A- ---\n",
    "    A_plus = 0\n",
    "    A_minus = 0\n",
    "    if prob_not_zero > 0: # Evitar división por cero si todos los puntos son 0\n",
    "         A_plus = prob_pos * (1 + prob_zero / prob_not_zero)\n",
    "         A_minus = prob_neg * (1 + prob_zero / prob_not_zero)\n",
    "\n",
    "    # --- Paso 3: Calcular la diferencia normalizada en el tamaño de las poblaciones ---\n",
    "    delta_A = A_plus - A_minus\n",
    "\n",
    "    # --- Paso 4: Calcular los centros de gravedad gc+ y gc- ---\n",
    "    # La integral de p(x)*x dx para una variable discreta es sumatorio(x * p(x))\n",
    "    # sum_{x en V, x>0} x * p(x) = sum_{x perteneciente a V, x>0} x * (# de x en V / N)\n",
    "    # Esto es igual a (sumatorio de todos los valores positivos en V incluyendo repeticiones) / N\n",
    "    sum_pos_values = sum(v for v in V if v > 0)\n",
    "    sum_neg_values = sum(v for v in V if v < 0)\n",
    "\n",
    "    numerator_gc_pos = sum_pos_values / N\n",
    "    numerator_gc_minus = sum_neg_values / N\n",
    "\n",
    "    gc_plus = 0\n",
    "    if A_plus > 0:\n",
    "        gc_plus = numerator_gc_pos / A_plus\n",
    "\n",
    "    gc_minus = 0\n",
    "    if A_minus > 0:\n",
    "        gc_minus = numerator_gc_minus / A_minus\n",
    "\n",
    "    # --- Paso 5: Calcular la distancia normalizada entre los centros de gravedad ---\n",
    "    d = abs(gc_plus - gc_minus) / 2.0 # Dividido por el rango total [-1, 1] que es 2\n",
    "\n",
    "    # --- Paso 6: Calcular el Índice de Polarización Gravitatoria (IPG) ---\n",
    "    mu = (1 - abs(delta_A)) * d\n",
    "\n",
    "    return mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso:\n",
    "V_ejemplo = [-1, -1, -0.5, 0, 0.5, 0.5, 0.5, 1, 1]\n",
    "ipg_calculado = calcular_IPG(V_ejemplo)\n",
    "print(f\"El vector V es: {V_ejemplo}\")\n",
    "print(f\"El IPG calculado es: {ipg_calculado}\")\n",
    "\n",
    "V_ejemplo_neutro = [0, 0, 0, 0, 0]\n",
    "ipg_neutro = calcular_IPG(V_ejemplo_neutro)\n",
    "print(f\"\\nEl vector V es: {V_ejemplo_neutro}\")\n",
    "print(f\"El IPG calculado es: {ipg_neutro}\")\n",
    "\n",
    "V_ejemplo_polarizado = [-1, -1, -1, 1, 1, 1]\n",
    "ipg_polarizado = calcular_IPG(V_ejemplo_polarizado)\n",
    "print(f\"\\nEl vector V es: {V_ejemplo_polarizado}\")\n",
    "print(f\"El IPG calculado es: {ipg_polarizado}\")\n",
    "\n",
    "V_ejemplo_sesgado = [-1, -1, -1, -1, -1, 1]\n",
    "ipg_sesgado = calcular_IPG(V_ejemplo_sesgado)\n",
    "print(f\"\\nEl vector V es: {V_ejemplo_sesgado}\")\n",
    "print(f\"El IPG calculado es: {ipg_sesgado}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicación a nuestras muestras\n",
    "\n",
    "IPG_16 = calcular_IPG(V_16)\n",
    "IPG_20 = calcular_IPG(V_20)\n",
    "IPG_24 = calcular_IPG(V_24)\n",
    "\n",
    "print(IPG_16)\n",
    "print(IPG_20)\n",
    "print(IPG_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7377e",
   "metadata": {},
   "source": [
    "#### Índice de Foster y Wolfson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fb257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gini(x):\n",
    "    # Asegúrate de que no haya valores negativos\n",
    "    x = np.array(x)\n",
    "    if np.amin(x) < 0:\n",
    "        raise ValueError(\"Los valores no pueden ser negativos\")\n",
    "\n",
    "    # Si todo es cero, el Gini no está definido (retornamos 0 por convención)\n",
    "    if np.all(x == 0):\n",
    "        return 0.0\n",
    "\n",
    "    # Ordenamos\n",
    "    x_sorted = np.sort(x)\n",
    "    n = len(x)\n",
    "    index = np.arange(1, n + 1)\n",
    "\n",
    "    # Fórmula del índice de Gini\n",
    "    return (2 * np.sum(index * x_sorted)) / (n * np.sum(x_sorted)) - (n + 1) / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdaacfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def calcular_FW(V):\n",
    "    \"\"\"\n",
    "    Calcula el índice de Foster-Wolfson P para un vector discreto V,\n",
    "    asumiendo que V contiene valores en el intervalo [0, 1]\n",
    "\n",
    "    Args:\n",
    "        V: Un vector (lista o array de numpy) de valores numéricos\n",
    "           en el rango [0, 1]. \n",
    "\n",
    "    Returns:\n",
    "        El valor del índice P calculado. Retorna 0.0 si el vector\n",
    "        está vacío o tiene un solo elemento.\n",
    "    \"\"\"\n",
    "    V = np.array(V, dtype=float) # Asegurar tipo float\n",
    "    V = V[~np.isnan(V)] # Limpiar NaNs\n",
    "\n",
    "    N = len(V)\n",
    "\n",
    "    # Casos degenerados: vector vacío o un solo elemento\n",
    "    # En estos casos, la polarización es 0 y las fórmulas no aplican directamente.\n",
    "    if N == 0 or N == 1:\n",
    "        return 0.0\n",
    "\n",
    "    # 1. Calcular mu (media aritmética) y m (mediana) de toda la población\n",
    "    mu = np.mean(V)\n",
    "    m = np.median(V)\n",
    "\n",
    "    if mu == 0 or m == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # 2. Calcular G (Índice de Gini)\n",
    "    # Usamos scipy.stats.gini que espera datos no negativos, lo cual se cumple con [0, 1].\n",
    "    G = gini(V)\n",
    "\n",
    "    # 3. Calcular T = (mu+ - mu-) / mu\n",
    "    # Dividir V en dos grupos de tamaño N//2 y N - N//2 para calcular mu+ y mu-.\n",
    "    # Se sigue la regla de distribuir los valores iguales a la mediana para igualar tamaños.\n",
    "\n",
    "    V_sorted = np.sort(V)\n",
    "\n",
    "    # Número de elementos estrictamente por debajo/encima de la mediana *del conjunto completo*\n",
    "    count_lt_m = np.sum(V < m)\n",
    "    count_eq_m = np.sum(V == m)\n",
    "\n",
    "    # Determinar cuántos valores iguales a la mediana deben ir al grupo \"inferior\"\n",
    "    # para que este grupo alcance el tamaño N//2.\n",
    "    target_lower_size = N // 2\n",
    "    num_median_to_lower = target_lower_size - count_lt_m\n",
    "\n",
    "    # Asegurarse de que num_median_to_lower esté dentro del rango [0, count_eq_m]\n",
    "    # Esto debería cumplirse bajo la asunción de que m es la mediana real y N>=2,\n",
    "    # pero max/min añaden robustez.\n",
    "    num_median_to_lower = max(0, num_median_to_lower)\n",
    "    num_median_to_lower = min(count_eq_m, num_median_to_lower)\n",
    "\n",
    "    # El número de valores iguales a la mediana que van al grupo \"superior\"\n",
    "    num_median_to_upper = count_eq_m - num_median_to_lower\n",
    "\n",
    "    # Construir los dos sub-vectores para el cálculo de T\n",
    "    # Grupo inferior: todos los < m y los num_median_to_lower valores iguales a m\n",
    "    V_lower_for_T = np.concatenate((V_sorted[V_sorted < m], np.full(num_median_to_lower, m)))\n",
    "    # Grupo superior: num_median_to_upper valores iguales a m y todos los > m\n",
    "    V_upper_for_T = np.concatenate((np.full(num_median_to_upper, m), V_sorted[V_sorted > m]))\n",
    "\n",
    "    # Calcular mu- y mu+ (medias de los sub-vectores)\n",
    "    mu_minus = np.mean(V_lower_for_T)\n",
    "    mu_plus = np.mean(V_upper_for_T)\n",
    "\n",
    "    # Calcular T = (mu+ - mu-) / mu\n",
    "    T = (mu_plus - mu_minus) / mu\n",
    "\n",
    "    # 4. Calcular P = 2 * (mu / m) * (T - G)\n",
    "    P = 2.0 * (mu / m) * (T - G)\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96092ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ejemplos de uso ---\n",
    "\n",
    "# Ejemplo 1: Distribución bimodal simple en [0, 1]\n",
    "V_ejemplo_bimodal_0_1 = np.array([0.1, 0.1, 0.2, 0.8, 0.9, 0.9])\n",
    "print(\"--- Ejemplo 1 (Datos en [0, 1], Bimodal) ---\")\n",
    "print(f\"Vector V: {V_ejemplo_bimodal_0_1}\")\n",
    "ipg_fw_0_1_1 = calcular_FW(V_ejemplo_bimodal_0_1)\n",
    "print(f\"Foster-Wolfson P: {ipg_fw_0_1_1}\")\n",
    "print(f\"Media (mu): {np.mean(V_ejemplo_bimodal_0_1)}, Mediana (m): {np.median(V_ejemplo_bimodal_0_1)}\")\n",
    "\n",
    "# Ejemplo 2: Datos sesgados en [0, 1] con punto en la mediana\n",
    "V_ejemplo_sesgado_0_1 = np.array([0.1, 0.2, 0.3, 0.5, 0.5, 0.8, 0.9, 0.9])\n",
    "print(\"\\n--- Ejemplo 2 (Datos en [0, 1], Sesgado con punto en mediana) ---\")\n",
    "print(f\"Vector V: {V_ejemplo_sesgado_0_1}\")\n",
    "ipg_fw_0_1_2 = calcular_FW(V_ejemplo_sesgado_0_1)\n",
    "print(f\"Foster-Wolfson P: {ipg_fw_0_1_2}\")\n",
    "print(f\"Media (mu): {np.mean(V_ejemplo_sesgado_0_1)}, Mediana (m): {np.median(V_ejemplo_sesgado_0_1)}\")\n",
    "\n",
    "# Ejemplo 3: Todos los puntos son iguales (pero no 0)\n",
    "V_ejemplo_iguales_0_1 = np.array([0.7, 0.7, 0.7, 0.7])\n",
    "print(\"\\n--- Ejemplo 3 (Datos en [0, 1], Iguales y no cero) ---\")\n",
    "print(f\"Vector V: {V_ejemplo_iguales_0_1}\")\n",
    "ipg_fw_0_1_3 = calcular_FW(V_ejemplo_iguales_0_1)\n",
    "print(f\"Foster-Wolfson P: {ipg_fw_0_1_3}\") # Debería ser 0.0\n",
    "print(f\"Media (mu): {np.mean(V_ejemplo_iguales_0_1)}, Mediana (m): {np.median(V_ejemplo_iguales_0_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80153e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicación a nuestras muestras\n",
    "\n",
    "FW_16 = calcular_FW(V_16_traslacion)\n",
    "FW_20 = calcular_FW(V_20_traslacion)\n",
    "FW_24 = calcular_FW(V_24_traslacion)\n",
    "\n",
    "print(FW_16)\n",
    "print(FW_20)\n",
    "print(FW_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509eafa",
   "metadata": {},
   "source": [
    "#### Índice de Esteban y Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3fa3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def calcular_ER(V, alpha):\n",
    "    \"\"\"\n",
    "    Calcula el índice de polarización de Esteban-Ray para un vector V en [-1, 1].\n",
    "\n",
    "    Args:\n",
    "        V: Un vector de opiniones en el rango [-1, 1].\n",
    "        alpha: Hiperparámetro de la fórmula, debe estar en (0, 1.6].\n",
    "\n",
    "    Returns:\n",
    "        El valor del índice de polarización de Esteban-Ray calculado.\n",
    "        Retorna 0.0 en casos degenerados (vector vacío, una sola opinión o son todos cero).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si el valor de alpha está fuera del rango (0, 1.6].\n",
    "    \"\"\"\n",
    "    # 1. Validar alpha\n",
    "    if not (0 < alpha <= 1.6):\n",
    "        raise ValueError(\"El hiperparámetro alpha debe estar en el rango (0, 1.6]\")\n",
    "\n",
    "    V = np.array(V, dtype=float) # Asegurar tipo float\n",
    "    V = V[~np.isnan(V)] # Limpiar NaNs\n",
    "\n",
    "    N = len(V)\n",
    "\n",
    "    # Caso degenerado: vector vacío o un solo elemento\n",
    "    if N == 0 or N == 1:\n",
    "        return 0.0\n",
    "\n",
    "    # 2. Separar opiniones en < 0, == 0, > 0 y obtener sus recuentos\n",
    "    V_neg = V[V < 0]\n",
    "    V_zero = V[V == 0] # Aunque no usamos el array V_zero directamente en sumas, necesitamos su count\n",
    "    V_pos = V[V > 0]\n",
    "\n",
    "    count_neg = len(V_neg)\n",
    "    count_zero = len(V_zero)\n",
    "    count_pos = len(V_pos)\n",
    "\n",
    "    count_not_zero = count_neg + count_pos\n",
    "\n",
    "    # 3. Casos degenerados donde no se pueden formar dos grupos opuestos significativos\n",
    "    # Esto ocurre si no hay opiniones no-cero (todos son 0)\n",
    "    # O si todas las opiniones no-cero tienen el mismo signo (count_neg == 0 O count_pos == 0)\n",
    "    if count_not_zero == 0 or count_neg == 0 or count_pos == 0:\n",
    "        # En estos casos, no hay dos polos opuestos significativos para distribuir ceros.\n",
    "        # La polarización, según esta definición de grupos, es 0.\n",
    "        return 0.0\n",
    "\n",
    "    # 4. Distribuir las opiniones cero proporcionalmente a la división de los no-cero\n",
    "    # Proporción de opiniones no-cero que son negativas\n",
    "    prop_neg_among_not_zero = count_neg / count_not_zero\n",
    "    # Proporción de opiniones no-cero que son positivas\n",
    "    # prop_pos_among_not_zero = count_pos / count_not_zero # Esto es 1.0 - prop_neg_among_not_zero\n",
    "\n",
    "    # Número (posiblemente fraccionario) de opiniones cero asignadas a cada grupo\n",
    "    # Esta es la implementación de \"repartiendo proporcionalmente\" para los ceros\n",
    "    zeros_to_democrats = count_zero * prop_neg_among_not_zero\n",
    "    zeros_to_republicans = count_zero - zeros_to_democrats # Es equivalente a count_zero * (count_pos / count_not_zero)\n",
    "\n",
    "    # 5. Definir los dos grupos finales para la fórmula de ER y calcular sus propiedades\n",
    "    final_count_democrats = count_neg + zeros_to_democrats\n",
    "    final_count_republicans = count_pos + zeros_to_republicans\n",
    "\n",
    "    # Suma de valores en cada grupo. Las opiniones originales negativas/positivas contribuyen con su valor.\n",
    "    # Las opiniones cero contribuyen con 0 a la suma, independientemente de cuántas vayan a cada grupo.\n",
    "    sum_democrats_values = np.sum(V_neg)\n",
    "    sum_republicans_values = np.sum(V_pos)\n",
    "\n",
    "    # 6. Calcular proporciones (pi) y medias (x, y) para la fórmula de Esteban-Ray\n",
    "    pi = final_count_democrats / N\n",
    "    one_minus_pi = final_count_republicans / N # Es 1.0 - pi, calculado para simetría\n",
    "\n",
    "    # Calcular medias x e y. Dividir suma de valores por el tamaño final (conceptual) del grupo.\n",
    "    x = sum_democrats_values / final_count_democrats\n",
    "    y = sum_republicans_values / final_count_republicans\n",
    "\n",
    "    # 7. Calcular el índice de polarización P (Esteban-Ray fórmula)\n",
    "    # pi^(1+alpha)*(1-pi) + (1-pi)^(1+alpha)*pi\n",
    "    # pi y 1-pi están estrictamente entre 0 y 1 en este punto, por lo que las potencias son válidas.\n",
    "    term_bracket = pi**(1.0 + alpha) * (1.0 - pi) + (1.0 - pi)**(1.0 + alpha) * pi\n",
    "\n",
    "    # La diferencia entre las medias de los grupos.\n",
    "    # x será la media de valores <= 0 (negativos y/o ceros).\n",
    "    # y será la media de valores >= 0 (positivos y/o ceros).\n",
    "    difference_in_means = y - x\n",
    "\n",
    "    # Fórmula final del Índice de Polarización P\n",
    "    P = term_bracket * difference_in_means\n",
    "\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ejemplos de uso ---\n",
    "\n",
    "# Ejemplo 1: Bipolarización fuerte con algunos neutros\n",
    "V_ejemplo_1 = np.array([-1, -1, -0.5, 0, 0, 0.5, 1, 1])\n",
    "alpha_ejemplo = 1.0\n",
    "print(\"--- Ejemplo 1 (Esteban-Ray con distribución proporcional de ceros) ---\")\n",
    "print(f\"Vector V: {V_ejemplo_1}\")\n",
    "print(f\"Alpha: {alpha_ejemplo}\")\n",
    "er_prop_zeros_1 = calcular_ER(V_ejemplo_1, alpha_ejemplo)\n",
    "print(f\"Esteban-Ray P: {er_prop_zeros_1}\")\n",
    "\n",
    "\n",
    "# Ejemplo 2: Bipolarización fuerte sin neutros (Igual que antes)\n",
    "V_ejemplo_2 = np.array([-1, -1, -1, 1, 1, 1])\n",
    "alpha_ejemplo = 0.5\n",
    "print(f\"\\n--- Ejemplo 2 (Solo opiniones negativas y positivas) ---\")\n",
    "print(f\"Vector V: {V_ejemplo_2}\")\n",
    "print(f\"Alpha: {alpha_ejemplo}\")\n",
    "er_prop_zeros_2 = calcular_ER(V_ejemplo_2, alpha_ejemplo)\n",
    "print(f\"Esteban-Ray P: {er_prop_zeros_2}\")\n",
    "\n",
    "# Ejemplo 3: Con ceros y grupos no vacíos (El mismo que el 1, pero ahora lo probamos de nuevo)\n",
    "V_ejemplo_5 = np.array([-1, -0.5, 0, 0, 0.5, 1])\n",
    "alpha_ejemplo = 1.0\n",
    "print(f\"\\n--- Ejemplo 5 (Con ceros, distribución proporcional) ---\")\n",
    "print(f\"Vector V: {V_ejemplo_5}\")\n",
    "print(f\"Alpha: {alpha_ejemplo}\")\n",
    "er_prop_zeros_5 = calcular_ER(V_ejemplo_5, alpha_ejemplo)\n",
    "print(f\"Esteban-Ray P: {er_prop_zeros_5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab10001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicación a nuestras muestras\n",
    "\n",
    "ER_16 = calcular_ER(V_16, 1.5)\n",
    "ER_20 = calcular_ER(V_20, 1.5)\n",
    "ER_24 = calcular_ER(V_24, 1.5)\n",
    "\n",
    "print(ER_16)\n",
    "print(ER_20)\n",
    "print(ER_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3630a9",
   "metadata": {},
   "source": [
    "#### Índice Beta de Polarización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def calcular_entropia_usuario(data_table):\n",
    "    \"\"\"\n",
    "    Calcula la firmeza de opinión (entropía normalizada h_i) para cada usuario\n",
    "    basándose en la distribución de etiquetas ('dem', 'neu', 'rep') en sus tweets.\n",
    "\n",
    "    Args:\n",
    "        data_table: Una estructura de tabla con columnas\n",
    "                    'User', 'tweet', 'Label'. Las etiquetas\n",
    "                    son 'dem', 'neu', o 'rep'.\n",
    "\n",
    "    Returns:\n",
    "        Un diccionario {user_id: h_i}, donde h_i es la entropía normalizada [0, 1].\n",
    "        Retorna 0.0 para usuarios sin tweets o con todos los tweets de la misma etiqueta.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si la tabla de datos no contiene las columnas requeridas.\n",
    "    \"\"\"\n",
    "    # Asegurar que la entrada es un DataFrame\n",
    "    if not isinstance(data_table, pd.DataFrame):\n",
    "        try:\n",
    "            data_table = pd.DataFrame(data_table)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"No se pudo convertir data_table a DataFrame: {e}\")\n",
    "\n",
    "    if data_table.empty:\n",
    "        return {}\n",
    "    \n",
    "    # Asegurar que las columnas necesarias existen\n",
    "    required_cols = ['User', 'Label']\n",
    "    if not all(col in data_table.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in data_table.columns]\n",
    "        raise ValueError(f\"data_table debe contener las columnas: {required_cols}. Faltan: {missing}\")\n",
    "    \n",
    "    # Definir todas las etiquetas posibles\n",
    "    all_labels = ['dem', 'neu', 'rep']\n",
    "\n",
    "    # Agrupar por usuario y contar etiquetas\n",
    "    # .unstack() convierte las etiquetas en columnas, fill_value=0 llena con ceros donde un usuario no tiene una etiqueta\n",
    "    user_label_counts = data_table.groupby('User')['Label'].value_counts().unstack(fill_value=0)\n",
    "    \n",
    "    # Asegurar que todas las columnas de etiquetas ('dem', 'neu', 'rep') existan, incluso si ningún usuario las tiene\n",
    "    for label in all_labels:\n",
    "        if label not in user_label_counts.columns:\n",
    "            user_label_counts[label] = 0\n",
    "\n",
    "    # Reordenar columnas para que siempre estén en el orden 'dem', 'neu', 'rep'\n",
    "    user_label_counts = user_label_counts[all_labels]\n",
    "\n",
    "    # Calcular el total de tweets por usuario\n",
    "    user_total_tweets = user_label_counts.sum(axis=1)\n",
    "    \n",
    "    # Calcular las proporciones p_ik. Evitar división por cero para usuarios sin tweets (aunque value_counts lo manejaría)\n",
    "    # Reemplazamos 0 por NaN en el total para que la división dé NaN, luego fillna(0) para que la entropía sea 0.\n",
    "    user_total_tweets_safe = user_total_tweets.replace(0, np.nan)\n",
    "    user_label_proportions = user_label_counts.div(user_total_tweets_safe, axis=0).fillna(0.0) # .fillna(0.0) si el total es 0\n",
    "    \n",
    "    # Asegúrate de que user_label_proportions es puramente numérico\n",
    "    ulp_values = user_label_proportions.astype(float).values \n",
    "\n",
    "    log_proportions = np.zeros_like(ulp_values)\n",
    "    mask = ulp_values > 0\n",
    "    log_proportions[mask] = np.log(ulp_values[mask])\n",
    "\n",
    "    p_log_p = ulp_values * log_proportions \n",
    "    # p_log_p ya no debería tener NaNs de 0*log(0), \n",
    "    # pero np.nan_to_num no haría daño si los datos originales tuvieran NaNs.\n",
    "    p_log_p = np.nan_to_num(p_log_p, nan=0.0) \n",
    "\n",
    "    # Sumar p_ik * log(p_ik) sobre las etiquetas para cada usuario\n",
    "    sum_p_log_p_per_user = p_log_p.sum(axis=1)\n",
    "\n",
    "    # Calcular entropía h_i = - (1 / log(3)) * sum(p_ik * log(p_ik)).\n",
    "    log3 = np.log(3)\n",
    "    \n",
    "    h_i = -sum_p_log_p_per_user / log3 # h_i es un array\n",
    "    \n",
    "    # Obtener los nombres de usuario del índice del DataFrame de proporciones\n",
    "    user_ids = user_label_proportions.index\n",
    "    \n",
    "    # Crear una Serie de Pandas para facilitar la conversión a diccionario\n",
    "    h_i_series = pd.Series(h_i, index=user_ids)\n",
    "\n",
    "    # El resultado como diccionario {user_id: h_i}\n",
    "    return h_i_series.to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b23cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_IB(V, beta, entropies_h_i, user_ids_for_V):\n",
    "    \"\"\"\n",
    "    Calcula el Índice Beta de polarización según las fórmulas proporcionadas,\n",
    "    incluyendo la distribución proporcional de usuarios cero y el uso de entropías.\n",
    "\n",
    "    Args:\n",
    "        V: Array de opiniones (vote_intention) en [-1, 1], alineado con user_ids_for_V.\n",
    "        beta: Parámetro de peso de cohesión [0, 1].\n",
    "        entropies_h_i: Diccionario {user_id: h_i} de entropías normalizadas por usuario,\n",
    "                       calculadas por calcular_entropia_usuario.\n",
    "        user_ids_for_V: Array de IDs de usuario, alineado con V (user_ids_for_V[k] es\n",
    "                        el ID del usuario cuya opinión es V[k]).\n",
    "\n",
    "    Returns:\n",
    "        El valor del Índice Beta de Polarización [0, 1]. Retorna 0.0 en casos degenerados.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Si beta está fuera del rango [0, 1].\n",
    "        ValueError: Si V y user_ids_for_V no tienen el mismo tamaño.\n",
    "        ValueError: Si falta la entropía para algún usuario que aparece en V.\n",
    "    \"\"\"\n",
    "    # 1. Validar beta\n",
    "    if not (0 <= beta <= 1):\n",
    "        raise ValueError(\"El parámetro beta debe estar en el rango [0, 1]\")\n",
    "\n",
    "    # Asegurar que las entradas sean arrays de numpy y tengan el mismo tamaño\n",
    "    V = np.array(V, dtype=float)\n",
    "    user_ids_for_V = np.array(user_ids_for_V)\n",
    "\n",
    "    if len(V) != len(user_ids_for_V):\n",
    "        raise ValueError(\"Los arrays V y user_ids_for_V deben tener el mismo tamaño.\")\n",
    "\n",
    "    # Limpiar NaNs de V y los user_ids correspondientes (manteniendo alineación)\n",
    "    valid_mask = ~np.isnan(V)\n",
    "    V = V[valid_mask]\n",
    "    user_ids_for_V = user_ids_for_V[valid_mask]\n",
    "\n",
    "    N = len(V) # Número total de usuarios válidos con opinión\n",
    "\n",
    "    # Casos degenerados: vector vacío o un solo usuario\n",
    "    if N == 0 or N == 1:\n",
    "        return 0.0\n",
    "\n",
    "    # 2. Separar usuarios por opinión: < 0, == 0, > 0\n",
    "    neg_mask = V < 0\n",
    "    zero_mask = V == 0\n",
    "    pos_mask = V > 0\n",
    "\n",
    "    users_neg = user_ids_for_V[neg_mask]\n",
    "    users_zero = user_ids_for_V[zero_mask]\n",
    "    users_pos = user_ids_for_V[pos_mask]\n",
    "\n",
    "    count_neg = len(users_neg)\n",
    "    count_zero = len(users_zero)\n",
    "    count_pos = len(users_pos)\n",
    "\n",
    "    count_not_zero = count_neg + count_pos\n",
    "\n",
    "    # 3. Casos degenerados donde no se pueden formar dos polos opuestos significativos\n",
    "    # Si no hay usuarios no-cero, o si todos los usuarios no-cero están en el mismo polo.\n",
    "    if count_not_zero == 0 or count_neg == 0 or count_pos == 0:\n",
    "        # No hay dos polos opuestos (D y R estrictos) entre los no-cero para distribuir los ceros.\n",
    "        # La polarización bipartidista es 0.\n",
    "        return 0.0\n",
    "\n",
    "    # 4. Distribuir los usuarios cero proporcionalmente entre los polos D y R.\n",
    "    # La proporción se basa en el tamaño relativo de los polos estrictos (no-cero).\n",
    "    prop_neg_among_not_zero = count_neg / count_not_zero\n",
    "\n",
    "    # Calcular cuántos usuarios cero van a cada polo. Redondeamos para obtener un número entero.\n",
    "    # La suma de los redondeados debe ser igual a count_zero.\n",
    "    num_zero_to_democrats = round(count_zero * prop_neg_among_not_zero)\n",
    "    num_zero_to_republicans = count_zero - num_zero_to_democrats # El resto para asegurar que suman count_zero\n",
    "\n",
    "    # 5. Definir los sets finales de usuarios para cada polo (D y R)\n",
    "    # Seleccionamos usuarios cero para D y R de forma determinística (ej. por orden en el array original users_zero).\n",
    "    users_zero_to_D = users_zero[:num_zero_to_democrats]\n",
    "    users_zero_to_R = users_zero[num_zero_to_democrats:] # Los restantes usuarios cero\n",
    "\n",
    "    # Los polos finales D y R son la unión de los usuarios estrictos y los cero asignados.\n",
    "    D_users = np.concatenate((users_neg, users_zero_to_D))\n",
    "    R_users = np.concatenate((users_pos, users_zero_to_R))\n",
    "\n",
    "    # abs_D = |D|, abs_R = |R|\n",
    "    abs_D = len(D_users)\n",
    "    abs_R = len(R_users)\n",
    "\n",
    "    if abs_D == 0 or abs_R == 0:\n",
    "         return 0.0\n",
    "\n",
    "    # 6. Calcular m_D y m_R (mediana de la intensidad absoluta dentro de cada polo)\n",
    "    # Obtener los valores de opinión V correspondientes a los usuarios en cada polo final\n",
    "    # Nota: V y user_ids_for_V están alineados. Usamos np.isin para filtrar V basado en los IDs de los polos.\n",
    "    V_D_values = V[np.isin(user_ids_for_V, D_users)]\n",
    "    V_R_values = V[np.isin(user_ids_for_V, R_users)]\n",
    "\n",
    "    # Calcular la mediana de los valores absolutos dentro de cada polo\n",
    "    # Los valores absolutos de los usuarios cero son |0| = 0.\n",
    "    m_D = np.median(np.abs(V_D_values))\n",
    "    m_R = np.median(np.abs(V_R_values))\n",
    "\n",
    "    # 7. Calcular H_D y H_R (cohesión interna / entropía media por polo)\n",
    "    # Sumar las entropías h_i para todos los usuarios en cada polo final\n",
    "    sum_h_D = 0.0\n",
    "    for user_id in D_users:\n",
    "        if user_id not in entropies_h_i:\n",
    "             # Todos los usuarios que aparecen en V deben tener una entropía calculada\n",
    "             raise ValueError(f\"Falta la entropía (h_i) para el usuario '{user_id}' en el diccionario provided.\")\n",
    "        sum_h_D += entropies_h_i[user_id]\n",
    "\n",
    "    sum_h_R = 0.0\n",
    "    for user_id in R_users:\n",
    "        if user_id not in entropies_h_i:\n",
    "             raise ValueError(f\"Falta la entropía (h_i) para el usuario '{user_id}' en el diccionario provided.\")\n",
    "        sum_h_R += entropies_h_i[user_id]\n",
    "\n",
    "    # Calcular las entropías medias (H_D, H_R)\n",
    "    # abs_D y abs_R son > 0 aquí.\n",
    "    H_D = sum_h_D / abs_D\n",
    "    H_R = sum_h_R / abs_R\n",
    "\n",
    "    # 8. Calcular p (fracción del bloque minoritario)\n",
    "    p = min(abs_D, abs_R) / N\n",
    "\n",
    "    # 9. Calcular el Índice Beta final\n",
    "    # |m_D| y |m_R| ya son >= 0 por np.abs()\n",
    "    median_distance_term = m_D + m_R\n",
    "\n",
    "    # Componente de cohesión [0, 1 - beta*1] => [0, 1] ya que beta está en [0, 1] y (H_D+H_R)/2 está en [0, 1]\n",
    "    cohesion_term = 1 - beta * (H_D + H_R) / 2\n",
    "\n",
    "    # Índice Beta\n",
    "    indice_beta = median_distance_term * cohesion_term * p\n",
    "\n",
    "    # El índice se define en [0, 1].\n",
    "    # max(|m_D|+|m_R|) = 2 (ej. todos -1 y todos 1, distribuidos los 0s).\n",
    "    # max(cohesion_term) = 1 (cuando beta=0 o H_D+H_R=0).\n",
    "    # max(p) = 0.5 (cuando |D|=|R|=N/2).\n",
    "    # Max Index = 2 * 1 * 0.5 = 1.0. El rango [0, 1] es correcto.\n",
    "\n",
    "    return indice_beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a7553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ejemplos de uso ---\n",
    "\n",
    "# 1. Crear datos de ejemplo\n",
    "# Tabla de datos para calcular entropía\n",
    "data_tweets_ejemplo = pd.DataFrame({\n",
    "    'User': ['user1', 'user1', 'user1', 'user2', 'user2', 'user3', 'user3', 'user3', 'user3', 'user4', 'user5', 'user5', 'user5', 'user5', 'user5', 'user6', 'user7'],\n",
    "    'tweet': ['t1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10', 't11', 't12', 't13', 't14', 't15', 't16', 't17'],\n",
    "    'Label': ['dem', 'dem', 'neu', 'rep', 'rep', 'dem', 'neu', 'rep', 'neu', 'dem', 'dem', 'dem', 'dem', 'rep', 'neu', 'neu', 'rep']\n",
    "})\n",
    "\n",
    "V_ejemplo = np.array([-0.8, 0.9, -0.1, -0.9, 0.0, 0.0, 0.0])\n",
    "users_ejemplo = np.array(['user1', 'user2', 'user3', 'user4', 'user5', 'user6', 'user7'])\n",
    "\n",
    "# 2. Calcular entropías h_i (Función auxiliar)\n",
    "entropies_ejemplo = calcular_entropia_usuario(data_tweets_ejemplo)\n",
    "print(\"Entropías calculadas (h_i):\")\n",
    "print(entropies_ejemplo)\n",
    "\n",
    "# 3. Calcular Índice Beta\n",
    "beta_ejemplo = 0.5\n",
    "print(f\"\\n--- Cálculo Índice Beta con beta={beta_ejemplo} ---\")\n",
    "print(f\"Vector V: {V_ejemplo}\")\n",
    "print(f\"Usuarios: {users_ejemplo}\")\n",
    "indice_beta_calculado = calcular_IB(V_ejemplo, beta_ejemplo, entropies_ejemplo, users_ejemplo)\n",
    "print(f\"Índice Beta calculado: {indice_beta_calculado}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56910a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicación a nuestras muestras\n",
    "\n",
    "IB_16 = calcular_IB(V_16, 0, calcular_entropia_usuario(data_table_16), V_df_16.index)\n",
    "IB_20 = calcular_IB(V_20, 0, calcular_entropia_usuario(data_table_20), V_df_20.index)\n",
    "IB_24 = calcular_IB(V_24, 0, calcular_entropia_usuario(data_table_24), V_df_24.index)\n",
    "\n",
    "print(IB_16)\n",
    "print(IB_20)\n",
    "print(IB_24)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
